# Polaris - DevPost Submission

## Project Name
**Polaris** — AI-Powered Personalized Book Synthesis

## Tagline
Take anyone to the cutting edge of any knowledge domain. In hours, not years.

---

## DevPost Description (~200 words)

Polaris orchestrates **four Gemini 3 capabilities** across a 16-stage neuro-symbolic pipeline to synthesize personalized, research-backed books in hours.

**Gemini Deep Research** (`deep-research-pro-preview`) discovers cutting-edge papers and frameworks *before* the outline is finalized — so every book reflects frontier knowledge, not yesterday's textbooks. This is critical: structure follows research, not the other way around.

**Gemini 3 Flash** powers 90+ type-safe DataModels built on the Synalinks neuro-symbolic framework — using Generators, Branch (reader mode decision: practitioner vs. academic vs. hybrid), and Decision (LLM-based paper-to-chapter matching) for hierarchical planning with self-critique loops and bottom-up content assembly. Every subsection receives full book context, preventing the fragmentation typical of LLM-generated content.

**Gemini 3 Pro Image** generates custom book covers and in-chapter illustrations, with 15 artistic styles and dynamic prompts tailored to each book's specific content.

**Gemini Search Grounding** powers a claim-first citation pipeline: claims are planned before writing, then verified against primary sources with confidence scoring. False citations are rejected — conservative accuracy over impressive-looking references.

The result: a 100-200 page book with verified citations, domain-specific examples, and cutting-edge research — synthesized from scratch for one reader's exact goal, background, domain, and learning style.

---

## How We Built It

Polaris has two main components: a **Python generation engine** and a **Next.js web platform**.

### Generation Engine (Python)

Built on the **Synalinks** neuro-symbolic LLM framework, which provides type-safe structured outputs via DataModels and intelligent control flow via Branch operators. The pipeline has 16 stages:

1. **Deep Research** — Gemini Deep Research API (Interactions API) discovers latest papers, frameworks, and field knowledge
2. **Book Vision** — Determines reader mode (practitioner/academic/hybrid) using Synalinks Branch, shaping the entire book's structure
3. **Outline Generation** — Multi-angle concept extraction with coverage verification loops
4. **Research-Informed Restructuring** — Papers from Deep Research reshape the outline before finalization
5. **Chapter Prioritization** — Role-tagged chapters (IMPLEMENTATION, LANDSCAPE, FRONTIERS, etc.) selected based on reader's goal
6. **Hierarchical Planning** — Book plan → chapter plans → section plans, each with self-critique loops
7. **Two-Level Research Distribution** — Each paper assigned to exactly ONE chapter, each subsection gets ONLY its assigned concepts — prevents repetition
8. **Stage 2 Research** — arXiv API fetching and optional knowledge graph integration via MCP
9. **Citation Pipeline** — Claims planned per subsection → verified with Gemini Search Grounding → injected as formatted references
10. **Content Generation** — Bottom-up assembly: atomic subsections → sections → chapters, with full book context at every level
11. **Illustrations** — Mermaid diagrams and AI-generated images via Gemini 3 Pro Image
12. **Cover Generation** — Dynamic cover prompts generated by Synalinks, rendered by Gemini 3 Pro Image
13. **PDF Assembly** — Markdown → HTML → PDF via WeasyPrint

All intermediates are cached, enabling resume from any stage. Served via **FastAPI** with background job tracking.

### Web Platform (Next.js)

- **Landing page** — Polaris brand with user stories showing diverse use cases (career changers, researchers, accessibility needs)
- **Book Builder** — 5-step guided form: Topic → Domain → Goal → Background → Focus areas
- **Generation tracking** — Real-time progress polling with stage indicators
- **Download** — PDF and Markdown formats

### Tech Stack
- **Synalinks** — Neuro-symbolic LLM framework (DataModels, Generators, Branch)
- **Gemini 3 Flash** — Content generation
- **Gemini 3 Pro Image** — Cover and illustration generation
- **Gemini Deep Research** — Cutting-edge paper discovery (Interactions API)
- **Gemini Search Grounding** — Citation verification
- **FastAPI** — REST API with job tracking
- **Next.js 14** — Web platform (App Router, Tailwind, Shadcn/ui, Framer Motion)
- **WeasyPrint** — PDF generation
- **arXiv API** — Stage 2 paper fetching

---

## What Makes It Different

### The Knowledge Pipeline Problem
Knowledge takes a decade to reach people: research → validation → practitioner adoption → cross-domain diffusion → textbooks → courses → accessibility. By the time it reaches a book, the frontier has moved.

### Our Solution
We collapse this entire pipeline to hours. But more importantly, we don't adapt existing content — we **synthesize from scratch** for one reader across infinite dimensions:

- **Their exact knowledge state** — what they know, what they think they know but don't
- **Their specific goal** — not "learn ML" but "build a trading bot using RL for illiquid markets"
- **Their professional domain** — every example speaks their language
- **Their native language** — full synthesis, not awkward translation
- **Their learning style** — visual, textual, example-driven, theory-first
- **Their constraints** — 20-minute focus windows, accessibility needs

### Technical Innovation
1. **Research-first structure** — Papers discovered BEFORE outline finalized (most tools do the opposite)
2. **Neuro-symbolic pipeline** — 30+ type-safe DataModels prevent the hallucination cascade that breaks long-form generation
3. **Two-level research distribution** — Solves the repetition problem in multi-chapter LLM content
4. **Reader mode intelligence** — LLM decides practitioner/academic/hybrid mode, reshaping the entire book
5. **Claim-first citations** — Plan what needs citing BEFORE writing, then verify with real sources

---

## Challenges We Ran Into

1. **Repetition at scale** — LLMs naturally repeat themselves across chapters. We solved this with two-level research distribution: each paper is assigned to exactly one chapter, each subsection gets only its unique concepts.

2. **Citation hallucination** — LLMs confidently cite papers that don't exist. Our claim-first pipeline plans claims before writing, then verifies each against primary sources using Gemini Search Grounding. We reject Wikipedia, blog posts, and secondary sources — only original papers and documentation qualify.

3. **Structural coherence** — A 200-page book generated subsection-by-subsection loses coherence. We solve this with hierarchical planning (book → chapter → section plans with critique loops) and full-context generation (every subsection sees the complete book plan).

4. **Deep Research integration** — The Gemini Deep Research API (Interactions API) is asynchronous with variable completion times. We built a polling wrapper with caching, parallel execution with semaphores, and graceful fallback when research is unavailable.

---

## What We Learned

- Type-safe structured outputs (via Synalinks DataModels) are essential for reliable multi-stage LLM pipelines — without them, errors cascade unpredictably
- Research-first architecture fundamentally changes output quality compared to generate-then-research
- Conservative citation verification (reject when uncertain) produces far more trustworthy results than aggressive citation
- The two-level distribution pattern (chapter-level assignment + subsection-level scoping) generalizes beyond books to any long-form structured generation

---

## What's Next

- **Multi-language synthesis** — Full book generation in any language
- **Interactive courses** — Learn-by-doing modules with exercises tailored to your level
- **Learning paths** — Multi-step curricula that adapt as you grow
- **Collaborative editing** — Human-in-the-loop refinement of generated books
- **Real-time updates** — Automatically refresh books when new research is published
