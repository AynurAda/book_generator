# Polaris - DevPost Submission

## Project Name
**Polaris** — AI-Powered Personalized Book Synthesis

## Tagline
Take anyone to the cutting edge of any knowledge domain. In hours, not years.

---

## DevPost Description (~200 words)

Polaris orchestrates **four Gemini 3 capabilities** across a 16-stage neuro-symbolic pipeline to synthesize personalized, research-backed books in hours.

**Gemini Deep Research** (`deep-research-pro-preview`) discovers cutting-edge papers and frameworks *before* the outline is finalized — so every book reflects frontier knowledge, not yesterday's textbooks. This is critical: structure follows research, not the other way around.

**Gemini 3 Flash** powers 90+ type-safe DataModels built on the Synalinks neuro-symbolic framework — using Generators, Branch (reader mode decision: practitioner vs. academic vs. hybrid), and Decision (LLM-based paper-to-chapter matching) for hierarchical planning with self-critique loops and bottom-up content assembly. Every subsection receives full book context, preventing the fragmentation typical of LLM-generated content.

**Gemini 3 Pro Image** generates custom book covers and in-chapter illustrations, with 15 artistic styles and dynamic prompts tailored to each book's specific content.

**Gemini Search Grounding** powers a claim-first citation pipeline: claims are planned before writing, then verified against primary sources with confidence scoring. False citations are rejected — conservative accuracy over impressive-looking references.

The result: a 100-200 page book with verified citations, domain-specific examples, and cutting-edge research — synthesized from scratch for one reader's exact goal, background, domain, and learning style.

---

## How We Built It

Polaris has two main components: a **Python generation engine** and a **Next.js web platform**.

### Generation Engine (Python)

Built on the **Synalinks** neuro-symbolic LLM framework, which provides type-safe structured outputs via DataModels (90+), intelligent control flow via Branch operators, and LLM-based routing via Decision. The pipeline has 16 stages:

1. **Deep Research** — Gemini Deep Research API (Interactions API) discovers latest papers, frameworks, and field knowledge
2. **Book Vision** — Determines reader mode (practitioner/academic/hybrid) using Synalinks Branch, shaping the entire book's structure
3. **Outline Generation** — Multi-angle concept extraction with coverage verification loops
4. **Research-Informed Restructuring** — Papers from Deep Research reshape the outline before finalization
5. **Chapter Prioritization** — Role-tagged chapters (IMPLEMENTATION, LANDSCAPE, FRONTIERS, etc.) selected based on reader's goal
6. **Hierarchical Planning** — Book plan → chapter plans → section plans, each with self-critique loops
7. **Two-Level Research Distribution** — Each paper assigned to exactly ONE chapter, each subsection gets ONLY its assigned concepts — prevents repetition
8. **Stage 2 Research** — arXiv ID resolution via Gemini Search, PDF extraction, and optional knowledge graph integration via MCP (mcp-graphiti)
9. **Citation Pipeline** — Claims planned per subsection → verified with Gemini Search Grounding → injected as formatted references
10. **Content Generation** — Bottom-up assembly: atomic subsections → sections → chapters, with full book context at every level
11. **Illustrations** — Mermaid diagrams and AI-generated images via Gemini 3 Pro Image
12. **Cover Generation** — Dynamic cover prompts generated by Synalinks, rendered by Gemini 3 Pro Image
13. **PDF Assembly** — Markdown → HTML → PDF via WeasyPrint

All intermediates are cached, enabling resume from any stage. Served via **FastAPI** with background job tracking.

### Web Platform (Next.js)

- **Landing page** — Polaris brand with user stories showing diverse use cases (career changers, researchers, accessibility needs)
- **Book Builder** — 5-step guided form: Topic → Domain → Goal → Background → Focus areas
- **Generation tracking** — Real-time progress polling with stage indicators
- **Download** — PDF and Markdown formats

### Tech Stack
- **Synalinks** — Neuro-symbolic LLM framework (90+ DataModels, Generator, Branch, Decision)
- **Gemini 3 Flash** — Content generation (via Synalinks)
- **Gemini 3 Pro Image** — Book covers (15 styles) and in-chapter illustrations
- **Gemini Deep Research** — Cutting-edge paper discovery (Interactions API, async polling)
- **Gemini Search Grounding** — Citation verification and arXiv ID resolution
- **FastAPI** — REST API with background job tracking
- **Next.js 14** — Web platform (App Router, Tailwind CSS, Shadcn/ui, Framer Motion)
- **WeasyPrint** — PDF generation with LaTeX math support
- **arXiv API + arxiv2text** — Stage 2 paper fetching and full-text PDF extraction
- **mcp-graphiti** — Optional knowledge graph via MCP protocol (Neo4j backend)

---

## Demo Scope & Roadmap

The demo showcases the **full end-to-end pipeline** — from topic input to downloadable PDF. All 16 pipeline stages are implemented and functional. Here's what's live in the demo vs. what's architected for production:

| Feature | Demo Status | Notes |
|---------|------------|-------|
| **Gemini Deep Research** | Live | Discovers papers before outline finalization |
| **Gemini 3 Flash (90+ DataModels)** | Live | Full neuro-symbolic pipeline via Synalinks |
| **Gemini Search Grounding (Citations)** | Live | Claim-first verification pipeline |
| **Gemini 3 Pro Image (Covers)** | Live | 15 cover styles, dynamic prompts |
| **Reader Mode (Branch)** | Live | Practitioner/academic/hybrid decision |
| **Hierarchical Planning + Self-Critique** | Live | Book → chapter → section plans |
| **Two-Level Research Distribution** | Live | Paper and concept exclusivity |
| **Web Platform (Next.js + FastAPI)** | Live | Builder wizard, progress tracking, download |
| **In-Chapter Illustrations** | Implemented, disabled in demo | Mermaid diagrams + Gemini 3 Pro Image |
| **Knowledge Graph (mcp-graphiti)** | Implemented, requires infra | Needs Neo4j + mcp-graphiti server |
| **arXiv Full-Text Extraction** | Implemented | PDF download + arxiv2text parsing |

The **knowledge graph integration** (Stage 11) uses MCP protocol to connect to [mcp-graphiti](https://github.com/rawr-ai/mcp-graphiti) for building a persistent knowledge graph across research papers. The code is fully implemented (`book_generator/research/stage2.py`) but requires a Neo4j instance and mcp-graphiti server — disabled in the demo to avoid infrastructure dependencies.

**In-chapter illustrations** are fully implemented (`book_generator/illustrations.py`) with both Mermaid diagram generation and Gemini 3 Pro Image rendering, but disabled in demo for faster generation times.

All features flagged above are **code-complete and tested** — they are configuration toggles, not missing functionality.

---

## What Makes It Different

### The Knowledge Pipeline Problem
Knowledge takes a decade to reach people: research → validation → practitioner adoption → cross-domain diffusion → textbooks → courses → accessibility. By the time it reaches a book, the frontier has moved.

### Our Solution
We collapse this entire pipeline to hours. But more importantly, we don't adapt existing content — we **synthesize from scratch** for one reader across infinite dimensions:

- **Their exact knowledge state** — what they know, what they think they know but don't
- **Their specific goal** — not "learn ML" but "build a trading bot using RL for illiquid markets"
- **Their professional domain** — every example speaks their language
- **Their native language** — full synthesis, not awkward translation
- **Their learning style** — visual, textual, example-driven, theory-first
- **Their constraints** — 20-minute focus windows, accessibility needs

### Technical Innovation
1. **Research-first structure** — Papers discovered BEFORE outline finalized (most tools do the opposite)
2. **Neuro-symbolic pipeline** — 90+ type-safe DataModels with Branch and Decision operators prevent the hallucination cascade that breaks long-form generation
3. **Two-level research distribution** — Solves the repetition problem in multi-chapter LLM content (concept exclusivity + domain exclusivity per subsection)
4. **Reader mode intelligence** — LLM decides practitioner/academic/hybrid mode via Branch, reshaping the entire book's organization and critique criteria
5. **Claim-first citations** — Plan what needs citing BEFORE writing, verify with Gemini Search Grounding against primary sources only
6. **LLM-based matching everywhere** — All paper/concept matching uses synalinks.Decision, never keyword matching

---

## Challenges We Ran Into

1. **Repetition at scale** — LLMs naturally repeat themselves across chapters. We solved this with two-level research distribution: each paper is assigned to exactly one chapter, each subsection gets only its unique concepts.

2. **Citation hallucination** — LLMs confidently cite papers that don't exist. Our claim-first pipeline plans claims before writing, then verifies each against primary sources using Gemini Search Grounding. We reject Wikipedia, blog posts, and secondary sources — only original papers and documentation qualify.

3. **Structural coherence** — A 200-page book generated subsection-by-subsection loses coherence. We solve this with hierarchical planning (book → chapter → section plans with critique loops) and full-context generation (every subsection sees the complete book plan).

4. **Deep Research integration** — The Gemini Deep Research API (Interactions API) is asynchronous with variable completion times. We built a polling wrapper with caching, parallel execution with semaphores, and graceful fallback when research is unavailable.

---

## What We Learned

- Type-safe structured outputs (via Synalinks DataModels) are essential for reliable multi-stage LLM pipelines — without them, errors cascade unpredictably across 16 stages
- Research-first architecture fundamentally changes output quality compared to generate-then-research
- Conservative citation verification (reject when uncertain) produces far more trustworthy results than aggressive citation — we reject Wikipedia, blogs, and secondary sources
- The two-level distribution pattern (chapter-level assignment + subsection-level scoping) generalizes beyond books to any long-form structured generation
- LLM-based matching (synalinks.Decision) vastly outperforms keyword matching for paper-to-chapter assignment
- Gemini with Google Search grounding is the most reliable method for resolving arXiv paper IDs — more reliable than fuzzy title matching

---

## What's Next

- **Multi-language synthesis** — Full book generation in any language
- **Interactive courses** — Learn-by-doing modules with exercises tailored to your level
- **Learning paths** — Multi-step curricula that adapt as you grow
- **Collaborative editing** — Human-in-the-loop refinement of generated books
- **Real-time updates** — Automatically refresh books when new research is published
